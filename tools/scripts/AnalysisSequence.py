import json
import re
import sys
from Bio.Seq import Seq
from Bio.SeqUtils.lcc import lcc_simp
from Bio.SeqUtils import gc_fraction as GC
import pandas as pd


class DNARepeatsFinder:
    def __init__(self, data_set=None, sequence=None):
        '''
        å¤Ÿæ¥å—å•ä¸ªåºåˆ—æˆ–è€…æ•°æ®é›†
        '''
        if data_set is not None:
            self.data_set = data_set
            self.suffix_arrays = {}
            self.lcps_arrays = {}

            # ä¸ºæ¯ä¸ªåºåˆ—æ„å»ºåç¼€æ•°ç»„å’ŒLCPæ•°ç»„,é¢„å…ˆè®¡ç®—æ¯æ¡åºåˆ—çš„åç¼€æ•°ç»„å’ŒLCPæ•°ç»„
            for index, row in data_set.iterrows():
                seq = row['sequence']
                self.suffix_arrays[index] = self.build_suffix_array(seq)
                self.lcps_arrays[index] = self.find_lcp(seq, self.suffix_arrays[index])
        elif sequence is not None:
            self.s = sequence
            self.sa = self.build_suffix_array(sequence)
            self.lcp = self.find_lcp(sequence, self.sa)

    @staticmethod
    def build_suffix_array(s):
        """Build suffix array of s in O(n log n)"""
        n = len(s)
        suffixes = sorted((s[i:], i) for i in range(n))
        sa = [suffix[1] for suffix in suffixes]
        return sa

    @staticmethod
    def find_lcp(s, sa):
        """Find the longest common prefix (LCP) array for `s` using the suffix array `sa`."""
        n = len(s)
        rank = [0] * n
        lcp = [0] * n

        for i in range(n):
            rank[sa[i]] = i

        h = 0
        for i in range(n):
            if rank[i] > 0:
                j = sa[rank[i] - 1]
                while (i + h < n) and (j + h < n) and (s[i + h] == s[j + h]):
                    h += 1
                lcp[rank[i]] = h
                if h > 0:
                    h -= 1
        return lcp
    
    def _get_sequence_data(self, index=None):
        if index is not None:
            return self.data_set.loc[index, 'sequence'], self.suffix_arrays[index], self.lcps_arrays[index]
        return self.s, self.sa, self.lcp
    
    def is_homopolymer(self, unit):
        '''Check if a given DNA sequence is a homopolymer (consisting of only one type of base).'''
        return len(set(unit)) == 1

    @staticmethod
    def gc_percent(seq):
        """Return GC percentage with two decimal places."""
        return round(GC(seq) * 100, 2)

    def calculate_homopolymer_penalty_score(self, length):
        # ä¿ç•™ 1 ä½å°æ•°
        return round((length * 10) if length > 10 else float(length), 1)
    
    def calculate_tandem_repeats_penalty_score(self, length):
        return round((length - 15) / 2 if length > 15 else 0, 1)
    
    def calculate_dispersed_repeats_penalty_score(self, length, count):
        return round(((length - 15) / 2 if length > 15 else 0) * count, 1)
    
    def calculate_palindrome_repeats_penalty_score(self, length):
        return round((length - 15) / 2 if length > 15 else 0, 1)
    
    def calculate_inverted_repeats_penalty_score(self, length, max_distance):
        return round((length - 10) - (max_distance-20), 1)
    
    def calculate_WS_motifs_penalty_score(self, length, min_len=8):
        return round((length - min_len + 1) / 2 , 1)
    
    def calculate_local_gc_penalty_score(self, gc_content, min_GC_threshold=None, max_GC_threshold=None):
        if min_GC_threshold is not None and gc_content <= min_GC_threshold:
            penalty_score = round(abs(min_GC_threshold - gc_content + 1) /10, 1)
        elif max_GC_threshold is not None and gc_content >= max_GC_threshold:
            penalty_score = round(abs(gc_content - max_GC_threshold + 1) /10, 1)
        else:
            # è¡¨ç¤ºä¸åœ¨èŒƒå›´å†…
            penalty_score = 0
        return penalty_score

    # 1) Tandem Repeats
    def find_tandem_repeats(self, index=None, min_unit=3, min_copies=4, max_mismatch=1):
        """
        æŸ¥æ‰¾ä¸²è”é‡å¤åºåˆ—
        å‚æ•°:
            min_unit: æœ€å°é‡å¤å•å…ƒé•¿åº¦ï¼Œé»˜è®¤3
            min_copies: æœ€å°é‡å¤æ¬¡æ•°ï¼Œé»˜è®¤4
            max_mismatch: å…è®¸çš„æœ€å¤§é”™é…æ•°ï¼Œé»˜è®¤1ï¼ˆè€ƒè™‘ä¸­é—´ mismatch çš„æƒ…å†µï¼‰
        ç½šåˆ†è§„åˆ™: (total_length - 15) / 2 if length > 15 else 0
        """
        s, sa, lcp = self._get_sequence_data(index)
        n = len(s)

        repeats_temp = []
        checked_positions = set()  # é¿å…é‡å¤æ£€æŸ¥

        # ğŸ”§ å®Œå…¨é‡å†™ï¼šæ­£ç¡®çš„ä¸²è”é‡å¤æ£€æµ‹é€»è¾‘
        # éå†åºåˆ—çš„æ¯ä¸ªä½ç½®ï¼ˆä¸ä½¿ç”¨suffix arrayçš„æ–¹å¼ï¼Œç›´æ¥éå†ï¼‰
        MAX_UNIT_LENGTH = 50

        for start_pos in range(n):
            if start_pos in checked_positions:
                continue

            # å°è¯•ä¸åŒçš„é‡å¤å•å…ƒé•¿åº¦
            for unit_length in range(min_unit, min(MAX_UNIT_LENGTH, (n - start_pos) // min_copies + 1)):
                repeat_unit = s[start_pos:start_pos + unit_length]

                # è·³è¿‡åŒèšç‰©ï¼ˆç”±å…¶ä»–ç®—æ³•å¤„ç†ï¼‰
                if self.is_homopolymer(repeat_unit):
                    continue

                # æ£€æŸ¥ä»å½“å‰ä½ç½®å¼€å§‹ï¼Œè¿™ä¸ªå•å…ƒé‡å¤äº†å¤šå°‘æ¬¡
                repeat_count = 0
                total_mismatches = 0
                pos = start_pos

                while pos + unit_length <= n:
                    current_segment = s[pos:pos + unit_length]
                    # è®¡ç®—é”™é…æ•°
                    mismatches = sum(1 for k in range(unit_length) if current_segment[k] != repeat_unit[k])

                    if mismatches <= max_mismatch:
                        repeat_count += 1
                        total_mismatches += mismatches
                        pos += unit_length
                    else:
                        break

                # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿçš„é‡å¤
                if repeat_count >= min_copies:
                    actual_length = repeat_count * unit_length

                    # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ èº«ä»½é˜ˆå€¼æ£€æŸ¥ï¼Œé¿å…å‡é˜³æ€§
                    # è®¡ç®—æ€»ä½“èº«ä»½ç™¾åˆ†æ¯”ï¼ˆåŒ¹é…ç¢±åŸºæ•°/æ€»ç¢±åŸºæ•°ï¼‰
                    total_bases = actual_length
                    matching_bases = total_bases - total_mismatches
                    identity_percent = matching_bases / total_bases if total_bases > 0 else 0

                    # è¦æ±‚è‡³å°‘85%çš„èº«ä»½ï¼ˆå³æœ€å¤š15%é”™é…ç‡ï¼‰
                    # è¿™ç¡®ä¿æ£€æµ‹åˆ°çš„æ˜¯çœŸæ­£çš„ä¸²è”é‡å¤ï¼Œè€Œä¸æ˜¯éšæœºç›¸ä¼¼åºåˆ—
                    # çœŸæ­£çš„ä¸²è”é‡å¤åº”è¯¥æ˜¯å¤§éƒ¨åˆ†copyå®Œç¾åŒ¹é…ï¼Œåªæœ‰å°‘æ•°çªå˜
                    # ä¾‹å¦‚ï¼š18bpé‡å¤ï¼Œæœ€å¤šå…è®¸2ä¸ªé”™é… (88.9% identity) âœ“
                    #       18bpé‡å¤ï¼Œ3ä¸ªé”™é… (83.3% identity) âœ— å¤ªå¤šçªå˜
                    MIN_IDENTITY = 0.85

                    if identity_percent < MIN_IDENTITY:
                        # èº«ä»½å¤ªä½ï¼Œä¸æ˜¯çœŸæ­£çš„ä¸²è”é‡å¤ï¼Œè·³è¿‡
                        continue

                    end_pos = start_pos + actual_length - 1

                    repeats_temp.append({
                        'sequence': s[start_pos:start_pos + actual_length],
                        'start': start_pos,
                        'end': end_pos,
                        'mismatches': total_mismatches,
                        'unit': repeat_unit,
                        'unit_length': unit_length,
                    })

                    # æ ‡è®°è¿™äº›ä½ç½®å·²æ£€æŸ¥ï¼Œé¿å…é‡å¤
                    for p in range(start_pos, start_pos + actual_length):
                        checked_positions.add(p)

                    # æ‰¾åˆ°ä¸€ä¸ªé‡å¤åï¼Œè·³å‡ºunit_lengthå¾ªç¯
                    break

        # æ ¹æ®startä½ç½®å¯¹repeats_tempè¿›è¡Œæ’åº
        repeats_temp.sort(key=lambda x: x['start'])

        # ğŸ”§ ä¿®å¤ï¼šæ”¹è¿›åˆå¹¶é€»è¾‘ï¼Œåªåˆå¹¶ç›¸åŒé‡å¤å•å…ƒçš„é‡å¤
        repeats = []
        for repeat in repeats_temp:
            if repeats and repeats[-1]['end'] >= repeat['start'] - 1:
                # æ£€æŸ¥é‡å¤å•å…ƒæ˜¯å¦ç›¸åŒ
                if repeats[-1].get('unit') == repeat.get('unit'):
                    # ç›¸åŒé‡å¤å•å…ƒï¼Œå¯ä»¥åˆå¹¶
                    old_end = repeats[-1]['end']
                    new_end = max(old_end, repeat['end'])
                    repeats[-1]['end'] = new_end
                    repeats[-1]['sequence'] = s[repeats[-1]['start']:new_end + 1]
                else:
                    # ä¸åŒé‡å¤å•å…ƒï¼Œé€‰æ‹©æ›´é•¿çš„ä¿ç•™
                    if repeat['end'] - repeat['start'] > repeats[-1]['end'] - repeats[-1]['start']:
                        repeats[-1] = repeat
            else:
                repeats.append(repeat)

        for repeat in repeats:
            repeat['length'] = len(repeat['sequence'])
            repeat['gc_content'] = self.gc_percent(repeat['sequence'])
            repeat['penalty_score'] = self.calculate_tandem_repeats_penalty_score(repeat['length'])
            # ğŸ”§ ä¿®å¤ï¼šæ¸…ç†ä¸´æ—¶å­—æ®µ
            repeat.pop('unit', None)
            repeat.pop('unit_length', None)

        # ğŸ”§ è¿‡æ»¤ï¼šåªä¿ç•™é•¿åº¦>15bpçš„ä¸²è”é‡å¤ï¼ˆå› ä¸ºâ‰¤15bpçš„ç½šåˆ†éƒ½æ˜¯0ï¼Œä¸éœ€è¦æŠ¥å‘Šï¼‰
        repeats = [r for r in repeats if r['length'] > 15]

        return repeats

    # 2) Dispersed Repeats
    def merge_potential_repeats(self, potential_repeats):
        # æŒ‰ç…§åºåˆ—çš„èµ·å§‹ä½ç½®æ’åº
        sorted_repeats = sorted(potential_repeats.items(), key=lambda x: x[1][0])
        merged_repeats = {}

        for seq, positions in sorted_repeats:
            merged = False
            for merged_seq in list(merged_repeats.keys()):
                merged_positions = merged_repeats[merged_seq]

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆå¹¶ï¼šå¦‚æœåºåˆ—çš„èµ·å§‹ä½ç½®æ¥è¿‘å¹¶ä¸”åºåˆ—æœ‰é‡å æˆ–æ¥è¿‘
                if abs(positions[0] - merged_positions[-1]) <= len(seq):
                    # åˆå¹¶åºåˆ—ï¼šé€‰æ‹©æ›´é•¿çš„åºåˆ—ä¿ç•™ï¼Œå¹¶åˆå¹¶ä½ç½®
                    if len(seq) > len(merged_seq):
                        merged_repeats[seq] = merged_positions + positions
                        del merged_repeats[merged_seq]  # åˆ é™¤æ—§çš„çŸ­åºåˆ—
                    else:
                        merged_repeats[merged_seq].extend(positions)
                    merged_repeats[seq if len(seq) > len(merged_seq) else merged_seq] = sorted(set(merged_repeats[seq if len(seq) > len(merged_seq) else merged_seq]))  # å»é‡å¹¶æ’åº
                    merged = True
                    break

            if not merged:
                merged_repeats[seq] = positions

        return merged_repeats

    def find_dispersed_repeats(self, index=None, min_len=16):
        s, sa, lcp = self._get_sequence_data(index)
        
        repeats = []
        n = len(s)
        potential_repeats = {}

        # æ”¶é›†æ‰€æœ‰æ½œåœ¨çš„é‡å¤åºåˆ—
        for i in range(1, n):
            length = lcp[i]
            if length >= min_len:
                start_index = sa[i]
                seq = s[start_index:start_index + length]
                if seq not in potential_repeats:
                    potential_repeats[seq] = []
                potential_repeats[seq].append(start_index)
        
        merged_repeats = self.merge_potential_repeats(potential_repeats)
        # è½¬æ¢ä¸ºæœ€ç»ˆè¾“å‡ºæ ¼å¼
        for seq, positions in merged_repeats.items():
            # æ ¹æ®åºåˆ—å»é‡æ–°æ‰¾åˆ°èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®
            matches = [match.start() for match in re.finditer(re.escape(seq), s)]
            if len(matches) >= 2:  # è‡³å°‘è¦æœ‰ä¸¤ä¸ªä»¥ä¸Šçš„é‡å¤ä½ç½®æ‰ç®—åˆ†æ•£é‡å¤
                distances = [matches[j] - matches[j - 1] for j in range(1, len(matches))]
                if all(d >= min_len for d in distances):
                    repeats.append({
                        'sequence': seq,
                        'start': matches,
                        'end': [match + len(seq) - 1 for match in matches],
                        'gc_content': self.gc_percent(seq),
                        'length': len(seq),
                        'penalty_score': self.calculate_dispersed_repeats_penalty_score(len(seq), len(matches))
                    })
        return repeats

    # 3) Palindrome Repeats
    def find_palindrome_repeats(self, index=None, min_len=15):
        """
        æŸ¥æ‰¾DNAå›æ–‡åºåˆ—ï¼ˆPalindromic Sequencesï¼‰

        DNAå›æ–‡å®šä¹‰ï¼šåºåˆ—ç­‰äºå…¶åå‘äº’è¡¥åºåˆ—
        ä¾‹å¦‚ï¼š
        - ATCGAT â†’ åå‘äº’è¡¥ ATCGAT (ç›¸åŒï¼Œæ˜¯DNAå›æ–‡)
        - GAATTC â†’ åå‘äº’è¡¥ GAATTC (EcoRIé™åˆ¶æ€§ä½ç‚¹)
        - CGGGGGC â†’ åå‘äº’è¡¥ GCCCCCG (ä¸åŒï¼Œä¸æ˜¯DNAå›æ–‡)

        æ³¨æ„ï¼šè¿™ä¸æ˜¯å­—ç¬¦ä¸²å›æ–‡ï¼å­—ç¬¦ä¸²å›æ–‡å¦‚ "ABCBA"ï¼Œä½†DNAå›æ–‡åŸºäºç¢±åŸºäº’è¡¥é…å¯¹ã€‚

        å‚æ•°:
            min_len: æœ€å°å›æ–‡é•¿åº¦ï¼Œé»˜è®¤15 (å¿…é¡»æ˜¯å¶æ•°)
        æ’é™¤è§„åˆ™: æ’é™¤ä¸¤ä¸ªç¢±åŸºäº¤æ›¿ç»„åˆçš„ repeatsï¼ˆå¦‚ ATATATATï¼‰ï¼Œé•¿åº¦â‰¥8
        ç½šåˆ†è§„åˆ™: (length - 15) / 2 if length > 15 else 0
        """
        s, sa, lcp = self._get_sequence_data(index)

        n = len(s)
        palindromes = []
        excluded_combinations = {"AT", "TA", "CG", "GC", "AC", "CA", "GT", "TG"}

        # ğŸ”§ ä¿®å¤ï¼šDNAå›æ–‡å¿…é¡»æ˜¯å¶æ•°é•¿åº¦ï¼Œç¡®ä¿ min_len æ˜¯å¶æ•°
        if min_len % 2 != 0:
            min_len += 1  # å¦‚æœæ˜¯å¥‡æ•°ï¼Œè°ƒæ•´ä¸ºä¸‹ä¸€ä¸ªå¶æ•°

        def is_excluded(sequence):
            """
            æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ï¼š
            - é•¿åº¦â‰¥8
            - åªåŒ…å«ä¸¤ç§ç¢±åŸº
            - è¿™ä¸¤ç§ç¢±åŸºäº¤æ›¿å‡ºç°ï¼ˆå¦‚ ATATAT æˆ– CGCGCGï¼‰
            """
            if len(sequence) < 8:
                return False

            unique_bases = set(sequence)
            if len(unique_bases) != 2:
                return False

            # æ£€æŸ¥å‰ä¸¤ä¸ªå­—ç¬¦çš„ç»„åˆæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
            if sequence[:2] not in excluded_combinations:
                return False

            # æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ›¿æ¨¡å¼
            pattern = sequence[:2]
            expected = (pattern * (len(sequence) // 2 + 1))[:len(sequence)]
            return sequence == expected

        def reverse_complement(seq):
            """è®¡ç®—åå‘äº’è¡¥åºåˆ—"""
            complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}
            return ''.join(complement[base] for base in reversed(seq.upper()))

        def is_dna_palindrome(seq):
            """æ£€æŸ¥æ˜¯å¦æ˜¯DNAå›æ–‡ï¼ˆåºåˆ— == åå‘äº’è¡¥ï¼‰"""
            return seq == reverse_complement(seq)

        # éå†æ‰€æœ‰å¯èƒ½çš„å­åºåˆ—ï¼ŒæŸ¥æ‰¾DNAå›æ–‡
        # DNAå›æ–‡å¿…é¡»æ˜¯å¶æ•°é•¿åº¦ï¼ˆå› ä¸ºåºåˆ—å¿…é¡»ç­‰äºå…¶åå‘äº’è¡¥ï¼‰
        for start in range(n - min_len + 1):
            # åªæ£€æŸ¥å¶æ•°é•¿åº¦çš„åºåˆ—
            for length in range(min_len, min(n - start + 1, 200), 2):  # é™åˆ¶æœ€å¤§é•¿åº¦200ä»¥æé«˜æ•ˆç‡
                end = start + length - 1
                seq = s[start:end + 1]

                # æ£€æŸ¥æ˜¯å¦æ˜¯DNAå›æ–‡
                if is_dna_palindrome(seq):
                    # åº”ç”¨æ’é™¤è§„åˆ™
                    if not is_excluded(seq):
                        palindromes.append({
                            'sequence': seq,
                            'start': start,
                            'end': end,
                            'length': length,
                            'gc_content': self.gc_percent(seq),
                            'penalty_score': self.calculate_palindrome_repeats_penalty_score(length),
                        })

        # è¿‡æ»¤é‡å çš„å›æ–‡ï¼Œä¿ç•™è¾ƒé•¿çš„
        filtered_palindromes = self._filter_overlapping_palindromes(palindromes)

        # æŒ‰èµ·å§‹ä½ç½®æ’åº
        filtered_palindromes.sort(key=lambda x: x['start'])

        return filtered_palindromes

    def _filter_overlapping_palindromes(self, palindromes):
        """
        è¿‡æ»¤é‡å çš„å›æ–‡åºåˆ—ï¼Œä¿ç•™è¾ƒé•¿çš„
        ç­–ç•¥ï¼šå¦‚æœä¸¤ä¸ªå›æ–‡é‡å è¶…è¿‡50%ï¼Œåªä¿ç•™è¾ƒé•¿çš„
        """
        if not palindromes:
            return []

        # æŒ‰é•¿åº¦é™åºæ’åº
        sorted_palindromes = sorted(palindromes, key=lambda x: x['length'], reverse=True)

        selected = []
        for candidate in sorted_palindromes:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰æ‹©çš„å›æ–‡æ˜¾è‘—é‡å 
            is_overlapping = False

            for selected_item in selected:
                overlap = self._calculate_overlap(
                    candidate['start'], candidate['end'],
                    selected_item['start'], selected_item['end']
                )

                # è®¡ç®—é‡å æ¯”ä¾‹
                candidate_len = candidate['end'] - candidate['start'] + 1
                overlap_ratio = overlap / candidate_len if candidate_len > 0 else 0

                # å¦‚æœé‡å è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¾è‘—é‡å 
                if overlap_ratio > 0.5:
                    is_overlapping = True
                    break

            if not is_overlapping:
                selected.append(candidate)

        return selected

    # 4) Inverted Repeats (åŒ…å« Hairpin å’Œ Inverted Repeats)
    def find_inverted_repeats(self, index=None, min_stem_len=10):
        """
        æŸ¥æ‰¾å€’ç½®é‡å¤åºåˆ—ï¼ˆåŒ…æ‹¬ hairpin å’Œ inverted repeatsï¼‰
        å‚æ•°:
            min_stem_len: æœ€å° stem é•¿åº¦ï¼Œé»˜è®¤10

        åˆ†ç±»å’Œç½šåˆ†è§„åˆ™:
        1. Hairpin: Stem >= 10, loop 4-8 bp
           ç½šåˆ†: stem_length - 9

        2. Inverted Repeats: Stem >= 16, loop >= 8 or <= 3
           ç½šåˆ†: ((stem_length - 15) / 2) * count
        """
        s, sa, lcp = self._get_sequence_data(index)
        n = len(s)

        def reverse_complement(seq):
            complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}
            return ''.join(complement[base] for base in reversed(seq.upper()))

        hairpins = []
        inverted_repeats = []

        # ç»Ÿè®¡ç›¸åŒ stem åºåˆ—å‡ºç°çš„æ¬¡æ•°ï¼ˆç”¨äº inverted repeats çš„ countï¼‰
        stem_counts = {}

        # éå†åºåˆ—æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å€’ç½®é‡å¤
        for i in range(n):
            max_stem = min(n - i, 100)  # é™åˆ¶æœ€å¤§ stem é•¿åº¦ä»¥æé«˜æ•ˆç‡

            for stem_len in range(min_stem_len, max_stem):
                stem1 = s[i:i + stem_len]
                stem1_rc = reverse_complement(stem1)

                # åœ¨ stem1 ä¹‹åæŸ¥æ‰¾å…¶åå‘äº’è¡¥åºåˆ—
                search_start = i + stem_len

                # ä½¿ç”¨å­—ç¬¦ä¸²æŸ¥æ‰¾æ–¹æ³•å¯»æ‰¾åå‘äº’è¡¥åºåˆ—
                pos = search_start
                while pos < n:
                    idx = s.find(stem1_rc, pos)
                    if idx == -1:
                        break

                    # è®¡ç®— loop é•¿åº¦
                    loop_length = idx - (i + stem_len)

                    if loop_length < 0:  # é‡å çš„æƒ…å†µï¼Œè·³è¿‡
                        pos = idx + 1
                        continue

                    stem2_end = idx + stem_len

                    # åˆ†ç±»ä¸º hairpin æˆ– inverted repeats
                    if stem_len >= 10 and 4 <= loop_length <= 8:
                        # Hairpin ç»“æ„
                        penalty = stem_len - 9
                        hairpins.append({
                            'type': 'hairpin',
                            'stem_sequence': stem1,
                            'stem_length': stem_len,
                            'stem1_start': i,
                            'stem1_end': i + stem_len - 1,
                            'stem2_start': idx,
                            'stem2_end': stem2_end - 1,
                            'loop_sequence': s[i + stem_len:idx],
                            'loop_length': loop_length,
                            'full_sequence': s[i:stem2_end],
                            'penalty_score': round(penalty, 1),
                        })

                    elif stem_len >= 16 and (loop_length >= 8 or loop_length <= 3):
                        # Inverted Repeats ç»“æ„
                        # ç»Ÿè®¡è¿™ä¸ª stem åºåˆ—çš„å‡ºç°æ¬¡æ•°
                        stem_key = (stem1, stem_len)
                        if stem_key not in stem_counts:
                            stem_counts[stem_key] = []
                        stem_counts[stem_key].append({
                            'stem1_start': i,
                            'stem1_end': i + stem_len - 1,
                            'stem2_start': idx,
                            'stem2_end': stem2_end - 1,
                            'loop_length': loop_length,
                            'loop_sequence': s[i + stem_len:idx],
                            'full_sequence': s[i:stem2_end],
                        })

                    pos = idx + 1

        # å¤„ç† inverted repeatsï¼Œè®¡ç®—æ¯ä¸ª stem çš„ count å’Œç½šåˆ†
        for (stem_seq, stem_len), occurrences in stem_counts.items():
            count = len(occurrences)
            base_penalty = (stem_len - 15) / 2 if stem_len > 15 else 0
            total_penalty = base_penalty * count

            for occ in occurrences:
                inverted_repeats.append({
                    'type': 'inverted_repeat',
                    'stem_sequence': stem_seq,
                    'stem_length': stem_len,
                    'stem1_start': occ['stem1_start'],
                    'stem1_end': occ['stem1_end'],
                    'stem2_start': occ['stem2_start'],
                    'stem2_end': occ['stem2_end'],
                    'loop_sequence': occ['loop_sequence'],
                    'loop_length': occ['loop_length'],
                    'full_sequence': occ['full_sequence'],
                    'count': count,
                    'penalty_score': round(total_penalty, 1),
                })

        # åˆå¹¶ hairpins å’Œ inverted_repeats
        all_results = hairpins + inverted_repeats

        # è¿‡æ»¤é‡å çš„ç»“æ„ï¼Œä¿ç•™æœ€æ˜¾è‘—çš„
        filtered_results = self._filter_overlapping_inverted_repeats(all_results)

        # æŒ‰èµ·å§‹ä½ç½®æ’åº
        filtered_results.sort(key=lambda x: x['stem1_start'])

        return filtered_results

    def _filter_overlapping_inverted_repeats(self, results):
        """
        è¿‡æ»¤é‡å çš„å€’ç½®é‡å¤ç»“æ„ï¼Œä¿ç•™æœ€æ˜¾è‘—çš„
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä¿ç•™ stem æ›´é•¿çš„ç»“æ„
        2. å¯¹äºç›¸åŒèµ·å§‹ä½ç½®çš„ç»“æ„ï¼Œåªä¿ç•™æœ€é•¿çš„
        3. å¯¹äºæ˜¾è‘—é‡å ï¼ˆ>50% overlapï¼‰çš„ç»“æ„ï¼Œä¿ç•™æ›´é«˜åˆ†çš„
        """
        if not results:
            return []

        # æŒ‰ stem_length é™åºæ’åºï¼ˆä¼˜å…ˆä¿ç•™é•¿çš„ï¼‰, ç„¶åæŒ‰ penalty_score é™åº
        sorted_results = sorted(results,
                               key=lambda x: (x['stem_length'], x['penalty_score']),
                               reverse=True)

        selected = []

        for candidate in sorted_results:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²é€‰æ‹©çš„ç»“æ„æ˜¾è‘—é‡å 
            is_overlapping = False

            for selected_item in selected:
                if self._has_significant_overlap(candidate, selected_item):
                    is_overlapping = True
                    break

            if not is_overlapping:
                selected.append(candidate)

        return selected

    def _has_significant_overlap(self, struct1, struct2):
        """
        æ£€æŸ¥ä¸¤ä¸ªå€’ç½®é‡å¤ç»“æ„æ˜¯å¦æ˜¾è‘—é‡å 
        åˆ¤æ–­æ ‡å‡†ï¼š
        - å¦‚æœ stem1 åŒºåŸŸé‡å è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¾è‘—é‡å 
        - æˆ–è€… stem2 åŒºåŸŸé‡å è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¾è‘—é‡å 
        """
        # è®¡ç®— stem1 çš„é‡å 
        stem1_overlap = self._calculate_overlap(
            struct1['stem1_start'], struct1['stem1_end'],
            struct2['stem1_start'], struct2['stem1_end']
        )

        # è®¡ç®— stem2 çš„é‡å 
        stem2_overlap = self._calculate_overlap(
            struct1['stem2_start'], struct1['stem2_end'],
            struct2['stem2_start'], struct2['stem2_end']
        )

        # è®¡ç®—é‡å æ¯”ä¾‹
        len1 = struct1['stem1_end'] - struct1['stem1_start'] + 1
        len2 = struct2['stem1_end'] - struct2['stem1_start'] + 1

        stem1_overlap_ratio = stem1_overlap / min(len1, len2) if min(len1, len2) > 0 else 0
        stem2_overlap_ratio = stem2_overlap / min(len1, len2) if min(len1, len2) > 0 else 0

        # å¦‚æœä»»ä¸€åŒºåŸŸé‡å è¶…è¿‡50%ï¼Œè®¤ä¸ºæ˜¾è‘—é‡å 
        return stem1_overlap_ratio > 0.5 or stem2_overlap_ratio > 0.5

    def _calculate_overlap(self, start1, end1, start2, end2):
        """è®¡ç®—ä¸¤ä¸ªåŒºé—´çš„é‡å é•¿åº¦"""
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        return max(0, overlap_end - overlap_start + 1)

    # 5) Homopolymers
    def find_homopolymers(self, index=None, min_len=7):
        s, sa, lcp = self._get_sequence_data(index)
    
        homopolymers = []
        n = len(s)
        start = 0
        for i in range(1, n):
            if s[i] != s[i - 1]:
                length = i - start
                if length >= min_len:
                    seq = s[start:i]
                    homopolymers.append({
                        'sequence': seq,
                        'start': start,
                        'end': i - 1,
                        'length': length,
                        'penalty_score': self.calculate_homopolymer_penalty_score(length),
                    })
                start = i
        return homopolymers

    # 6) W8S8 Motifs
    def find_WS_motifs(self, index=None, min_W_length=8, min_S_length=8):
        s, sa, lcp = self._get_sequence_data(index)
        
        W_pattern = r'[AT]{' + str(min_W_length) + ',}'
        S_pattern = r'[GC]{' + str(min_S_length) + ',}'

        def find_motifs(pattern, motif_type):
            motifs = []
            for match in re.finditer(pattern, s):
                start, end = match.span()
                matched_sequence = s[start:end]
                matched_sequence2 = match.group(0)
                if not self.is_homopolymer(matched_sequence):
                    motifs.append({
                        'motif_type': motif_type,
                        'sequence': s[start:end],
                        'start': start,
                        'end': end - 1,
                        'length': end - start,
                        'penalty_score': self.calculate_WS_motifs_penalty_score((end - start), min_len=min_S_length),
                    })
            return motifs

        W_motifs = find_motifs(W_pattern, 'W' + str(min_W_length))
        S_motifs = find_motifs(S_pattern, 'S' + str(min_S_length))

        motifs = W_motifs + S_motifs
        return motifs

    # 7) Local GC Content by Window
    def find_high_gc_content(self, index=None, window_size=30, max_GC_content=80):
        s, sa, lcp = self._get_sequence_data(index)
        # print(f"Sequence: {s}")  # è¾“å‡ºåºåˆ—

        n = len(s)
        if n < window_size:
            gc_raw = GC(s) * 100
            gc_content = round(gc_raw, 2)
            # print(f"GC Content for entire sequence: {gc_content}")  # è¾“å‡ºæ•´ä¸ªåºåˆ—çš„GCå«é‡
            if gc_raw >= max_GC_content:
                penalty_score = self.calculate_local_gc_penalty_score(float(gc_raw), max_GC_content, max_GC_content)
                # print(f"Penalty Score for entire sequence: {penalty_score}")  # è¾“å‡ºæƒ©ç½šåˆ†æ•°
                return [{
                    'length': n,
                    'sequence': s,
                    'start': 0,
                    'end': n - 1,
                    'gc_content': gc_content,
                    'penalty_score': penalty_score,
                }]
            return []

        high_gc_contents_temp = []

        for i in range(n - window_size + 1):
            window = s[i:i + window_size]
            gc_raw = GC(window) * 100
            gc_content = round(gc_raw, 2)
            if gc_raw >= max_GC_content:
                penalty_score = self.calculate_local_gc_penalty_score(float(gc_raw), max_GC_content, max_GC_content)
                # print(f"Window {i}-{i + window_size - 1}: {window}, GC Content: {gc_content} Penalty Score for window {i}-{i + window_size - 1}: {penalty_score}")
                high_gc_contents_temp.append({
                    'sequence': window,
                    'start': i,
                    'end': i + window_size - 1,
                    'gc_content': gc_content,
                    'penalty_score': penalty_score,
                })

        high_gc_contents = self.merge_gc_windows(high_gc_contents_temp, s, 'highGC', min_GC_content=None, max_GC_content=max_GC_content)
        # print(f"Merged High GC Contents: {high_gc_contents}")  # è¾“å‡ºåˆå¹¶åçš„é«˜GCçª—å£

        return high_gc_contents
    
    def find_low_gc_content(self, index=None, window_size=30, min_GC_content=20):
        s, sa, lcp = self._get_sequence_data(index)

        n = len(s)
        if n < window_size:
            # å°†æ•´ä¸ªåºåˆ—ä½œä¸ºä¸€ä¸ªçª—å£
            gc_raw = GC(s)*100
            gc_content = round(gc_raw, 2)
            if gc_raw <= min_GC_content:
                return [{
                    'length': n,
                    'sequence': s,
                    'start': 0,
                    'end': n - 1,
                    'gc_content': gc_content,
                    'penalty_score': self.calculate_local_gc_penalty_score(float(gc_raw), min_GC_content, min_GC_content),
                }]
            return []

        low_gc_contents_temp = []

        for i in range(n - window_size + 1):
            window = s[i:i + window_size]
            gc_raw = GC(window)*100
            gc_content = round(gc_raw, 2)
            if gc_raw < min_GC_content:
                low_gc_contents_temp.append({
                    'sequence': window,
                    'start': i,
                    'end': i + window_size - 1,
                    'gc_content': gc_content,
                    'penalty_score': self.calculate_local_gc_penalty_score(float(gc_raw), min_GC_content, min_GC_content),
                })

        low_gc_contents = self.merge_gc_windows(low_gc_contents_temp, s, 'lowGC', min_GC_content=min_GC_content, max_GC_content=None)

        return low_gc_contents

    def merge_gc_windows(self, gc_contents_temp, sequence, gc_type, min_GC_content=None, max_GC_content=None):
        """åŸå§‹åˆå¹¶é€»è¾‘ï¼šåªè¦åŒºé—´é‡å æˆ–ç›¸é‚»å°±åˆå¹¶ã€‚"""
        gc_contents = []
        for gc_content in gc_contents_temp:
            if gc_contents and gc_contents[-1]['end'] >= gc_content['start'] - 1:  # è¡¨ç¤ºæœ‰é‡å æˆ–ç›¸é‚»
                old_end = gc_contents[-1]['end']
                new_end = max(old_end, gc_content['end'])
                gc_contents[-1]['end'] = new_end
                gc_contents[-1]['sequence'] = sequence[gc_contents[-1]['start']:new_end + 1]
                gc_contents[-1]['penalty_score'] += gc_content['penalty_score']  # ç´¯åŠ æƒ©ç½šåˆ†æ•°
            else:
                gc_contents.append(gc_content)

        for gc_content in gc_contents:
            gc_content['seqType'] = gc_type
            gc_content['length'] = len(gc_content['sequence'])
            gc_content['gc_content'] = self.gc_percent(gc_content['sequence'])
            # ä¿ç•™ 1 ä½å°æ•°çš„æ˜¾ç¤ºæ•ˆæœ
            gc_content['penalty_score'] = round(float(gc_content.get('penalty_score', 0)), 1)
        return gc_contents

    # 8) Dinucleotide Repeats
    def find_dinucleotide_repeats(self, index=None, threshold=12):
        s, sa, lcp = self._get_sequence_data(index)
        n = len(s)
        repeats = []

        i = 0
        while i < n - 1:
            dinucleotide = s[i:i+2]
            if len(dinucleotide) < 2:
                break
            
            repeat_length = 2
            j = i + 2
            while j < n and s[j:j+2] == dinucleotide:
                repeat_length += 2
                j += 2
            
            if repeat_length >= threshold:
                sequence = dinucleotide * (repeat_length // 2)
                # åˆ¤æ–­æ˜¯å¦ä¸ºhomopolymer
                if dinucleotide[0] != dinucleotide[1]:
                    repeats.append({
                        # 'seqType': 'dinucleotide_repeats',
                        'sequence': sequence,
                        'start': i,
                        'end': i + repeat_length - 1,
                        'length': repeat_length,
                        'gc_content': self.gc_percent(sequence),
                        'penalty_score': round(repeat_length / 2, 1)
                    })
            
            i += repeat_length  # è·³è¿‡å·²æ£€æµ‹åˆ°çš„é‡å¤åºåˆ—
        return repeats

    # 9) lcc
    def get_lcc(self, index=None):
        s, sa, lcp = self._get_sequence_data(index)
        n = len(s)
        lcc = 0
        dna_seq = Seq(s)
        dna_complexity = lcc_simp(dna_seq)

        return f'{dna_complexity:.2f}'


def convert_gene_table_to_RepeatsFinder_Format(gene_table, long_repeats_min_len=16, homopolymers_min_len=7,
                                               min_w_length=12, min_s_length=12, window_size=30,
                                               min_gc_content=20, max_gc_content=80,
                                               tandem_min_unit=3, tandem_min_copies=4, tandem_max_mismatch=1,
                                               palindrome_min_len=15, inverted_min_stem_len=10):
    """
    å°†åŸºå› è¡¨æ ¼è½¬æ¢ä¸º RepeatsFinder æ ¼å¼

    æ–°å¢å‚æ•°:
        tandem_min_unit: Tandem Repeats æœ€å°å•å…ƒé•¿åº¦ï¼Œé»˜è®¤3
        tandem_min_copies: Tandem Repeats æœ€å°é‡å¤æ¬¡æ•°ï¼Œé»˜è®¤4
        tandem_max_mismatch: Tandem Repeats æœ€å¤§é”™é…æ•°ï¼Œé»˜è®¤1
        palindrome_min_len: Palindrome Repeats æœ€å°é•¿åº¦ï¼Œé»˜è®¤15
        inverted_min_stem_len: Inverted Repeats æœ€å° stem é•¿åº¦ï¼Œé»˜è®¤10
    """
    finder_dataset = DNARepeatsFinder(data_set=gene_table)

    results = {}
    for index, row in gene_table.iterrows():
        gene_id = row['gene_id']
        results[gene_id] = {
            'LongRepeats': finder_dataset.find_dispersed_repeats(index=index, min_len=long_repeats_min_len),
            'Homopolymers': finder_dataset.find_homopolymers(index=index, min_len=homopolymers_min_len),
            'W12S12Motifs': finder_dataset.find_WS_motifs(index=index, min_W_length=min_w_length, min_S_length=min_s_length),
            'highGC': finder_dataset.find_high_gc_content(index=index, window_size=window_size, max_GC_content=max_gc_content),
            'lowGC': finder_dataset.find_low_gc_content(index=index, window_size=window_size, min_GC_content=min_gc_content),
            # 'LCC': finder_dataset.get_lcc(index=index),
            'doubleNT': finder_dataset.find_dinucleotide_repeats(index=index, threshold=12),
            # ========== æ–°å¢ä¸‰ä¸ªåˆ†ææ–¹æ³• ==========
            'TandemRepeats': finder_dataset.find_tandem_repeats(index=index, min_unit=tandem_min_unit,
                                                                min_copies=tandem_min_copies,
                                                                max_mismatch=tandem_max_mismatch),
            'PalindromeRepeats': finder_dataset.find_palindrome_repeats(index=index, min_len=palindrome_min_len),
            'InvertedRepeats': finder_dataset.find_inverted_repeats(index=index, min_stem_len=inverted_min_stem_len),
            # ====================================
            'length': len(finder_dataset._get_sequence_data(index)[0])
        }

    # print(json.dumps(results, indent=4))
    return results


# æ ¼å¼åŒ–å‡½æ•°ï¼šå°†ç‰¹å¾ä¿¡æ¯åˆå¹¶ä¸ºå­—ç¬¦ä¸²
def format_feature_data(feature_data, keys):
    if not feature_data:
        return ""
    values = []
    if isinstance(feature_data, list):  # å¤„ç†å¤šä¸ªå¯¹è±¡çš„æƒ…å†µ
        for item in feature_data:
            parts = []
            for key in keys:
                parts.append(f"{key}: {item.get(key, '')}")
            values.append(" | ".join(parts))
    else:
        for key in keys:
            if isinstance(feature_data[key], list):
                values.append(f"{key}: {', '.join(map(str, feature_data[key]))}")
            else:
                values.append(f"{key}: {feature_data[key]}")
    return "; ".join(values)


def calculate_total_penalty_score(feature_list):
    # å®šä¹‰å‡½æ•°æ¥è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ€» penalty score
    total = sum(float(item.get('penalty_score', 0)) for item in feature_list)
    return round(total, 1)

# --- æ–°å¢ï¼šç»Ÿè®¡å„ç‰¹å¾æ€»é•¿åº¦ ---
def calculate_total_feature_length(feature_list):
    """
    è®¡ç®—ä¸€ä¸ªç‰¹å¾åˆ—è¡¨çš„æ€»é•¿åº¦ã€‚
    - è‹¥ item["start"] æ˜¯åˆ—è¡¨ï¼Œåˆ™ repeats = len(item["start"])ï¼›
      å¦åˆ™ repeats = 1ã€‚
    - æ€»é•¿åº¦ = repeats Ã— item["length"]ï¼ˆç¼ºå¤±æ—¶æŒ‰ 0ï¼‰ã€‚
    """
    total_len = 0
    for item in feature_list:
        base_len = item.get("length", 0) or 0

        starts = item.get("start", [])
        repeats = len(starts) if isinstance(starts, list) and starts else 1

        total_len += base_len * repeats

    return total_len

# æ•°æ®å¤„ç†å‡½æ•°ï¼Œè¿”å›å¤„ç†åçš„ DataFrame
def process_gene_table_results(data):
    records = []
    for gene, details in data.items():
        # 1) å„ç‰¹å¾æƒ©ç½šåˆ†
        long_repeats_penalty = calculate_total_penalty_score(details.get("LongRepeats", []))
        homopolymers_penalty = calculate_total_penalty_score(details.get("Homopolymers", []))
        w12s12motifs_penalty = calculate_total_penalty_score(details.get("W12S12Motifs", []))
        highGC_penalty = calculate_total_penalty_score(details.get("highGC", []))
        lowGC_penalty = calculate_total_penalty_score(details.get("lowGC", []))
        doubleNT_penalty = calculate_total_penalty_score(details.get("doubleNT", []))
        # ========== æ–°å¢ä¸‰ä¸ªç‰¹å¾çš„æƒ©ç½šåˆ† ==========
        tandem_repeats_penalty = calculate_total_penalty_score(details.get("TandemRepeats", []))
        palindrome_repeats_penalty = calculate_total_penalty_score(details.get("PalindromeRepeats", []))
        inverted_repeats_penalty = calculate_total_penalty_score(details.get("InvertedRepeats", []))

        # 2) å„ç‰¹å¾æ€»é•¿åº¦
        long_repeats_len = calculate_total_feature_length(details.get("LongRepeats", []))
        homopolymers_len = calculate_total_feature_length(details.get("Homopolymers", []))
        w12s12motifs_len = calculate_total_feature_length(details.get("W12S12Motifs", []))
        highGC_len = calculate_total_feature_length(details.get("highGC", []))
        lowGC_len = calculate_total_feature_length(details.get("lowGC", []))
        doubleNT_len = calculate_total_feature_length(details.get("doubleNT", []))
        # ========== æ–°å¢ä¸‰ä¸ªç‰¹å¾çš„æ€»é•¿åº¦ ==========
        tandem_repeats_len = calculate_total_feature_length(details.get("TandemRepeats", []))
        palindrome_repeats_len = calculate_total_feature_length(details.get("PalindromeRepeats", []))
        # InvertedRepeats é•¿åº¦è®¡ç®—éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆstem1 + loop + stem2ï¼‰
        inverted_repeats_len = sum(
            item.get('stem_length', 0) * 2 + item.get('loop_length', 0)
            for item in details.get("InvertedRepeats", [])
        )

        record = {
            "GeneName": gene,
            "Total_Length": details.get("length"),
            # "LCC": details.get("LCC"),
            "LongRepeats": format_feature_data(details.get("LongRepeats", []), ["sequence", "start", "end", "length", "gc_content"]),
            "LongRepeats_penalty_score": long_repeats_penalty,
            "Homopolymers": format_feature_data(details.get("Homopolymers", []), ["sequence", "start", "end", "length"]),
            "Homopolymers_penalty_score": homopolymers_penalty,
            "W12S12Motifs": format_feature_data(details.get("W12S12Motifs", []), ["sequence", "start", "end", "length"]),
            "W12S12Motifs_penalty_score": w12s12motifs_penalty,
            "HighGC": format_feature_data(details.get("highGC", []), ["sequence", "start", "end", "length", "gc_content"]),
            "HighGC_penalty_score": highGC_penalty,
            "LowGC": format_feature_data(details.get("lowGC", []), ["sequence", "start", "end", "length", "gc_content"]),
            "LowGC_penalty_score": lowGC_penalty,
            "DoubleNT": format_feature_data(details.get("doubleNT", []), ["sequence", "start", "end", "length", "gc_content"]),
            "DoubleNT_penalty_score": doubleNT_penalty,
            # ========== æ–°å¢ä¸‰ä¸ªç‰¹å¾ ==========
            "TandemRepeats": format_feature_data(details.get("TandemRepeats", []), ["sequence", "start", "end", "length", "gc_content"]),
            "TandemRepeats_penalty_score": tandem_repeats_penalty,
            "PalindromeRepeats": format_feature_data(details.get("PalindromeRepeats", []), ["sequence", "start", "end", "length", "gc_content"]),
            "PalindromeRepeats_penalty_score": palindrome_repeats_penalty,
            "InvertedRepeats": format_feature_data(details.get("InvertedRepeats", []),
                                                    ["type", "stem_sequence", "stem_length", "stem1_start", "stem1_end",
                                                     "stem2_start", "stem2_end", "loop_length", "loop_sequence"]),
            "InvertedRepeats_penalty_score": inverted_repeats_penalty,
            # ====================================
            # ------------- â˜… æ–°å¢çš„æ€»é•¿åº¦åˆ— â˜… -------------
            "LongRepeats_total_length": long_repeats_len,
            "Homopolymers_total_length": homopolymers_len,
            "W12S12Motifs_total_length": w12s12motifs_len,
            "HighGC_total_length": highGC_len,
            "LowGC_total_length": lowGC_len,
            "DoubleNT_total_length": doubleNT_len,
            # ========== æ–°å¢ä¸‰ä¸ªç‰¹å¾çš„æ€»é•¿åº¦ ==========
            "TandemRepeats_total_length": tandem_repeats_len,
            "PalindromeRepeats_total_length": palindrome_repeats_len,
            "InvertedRepeats_total_length": inverted_repeats_len,
            # ====================================
        }
        records.append(record)

    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame(records)

    return df

if __name__ == '__main__':
    input_file = sys.argv[1]
    if input_file.endswith('.xlsx'):
        gene_table = pd.read_excel(input_file)
    elif input_file.endswith('.csv'):
        gene_table = pd.read_csv(input_file)
    else:
        raise ValueError("Invalid file format. Please provide a .csv or .xlsx file")
    
    # gene_table['gene_id'] = gene_table['GeneName']
    # gene_table['sequence'] = gene_table['FullSeqREAL']

    data = convert_gene_table_to_RepeatsFinder_Format(gene_table)
    print(data)
    # å¤„ç†æ•°æ®å¹¶è·å–ç»“æœ DataFrame
    result_df = process_gene_table_results(data)
    print(result_df[['GeneName', 'Total_Length', 'LongRepeats_total_length', 'LongRepeats',]])
    # åˆå¹¶åŸå§‹è¡¨æ ¼
    # df = gene_table.merge(result_df, on='gene_id', how='left')
    # # åˆ é™¤ä¸éœ€è¦çš„åˆ—
    # df.drop(['gene_id', 'sequence'], axis=1, inplace=True)
    # # ä¿å­˜ç»“æœ
    # output_file = input_file.replace('.xlsx', '_output.txt').replace('.csv', '_output.txt')
